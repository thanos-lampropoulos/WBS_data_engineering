{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba54479d-dd34-4d92-8f7b-2e918823e7ab",
   "metadata": {},
   "source": [
    "### <ins>This notebook contains the following functions:</ins>\n",
    "\n",
    "(function names can be clicked)\n",
    "\n",
    "### [`local_mysql_Gans()`](#local_mysql_Gans) --> contains the credentials for accessing the local sql_Gans MySQL schema.\n",
    "### [`city_data(cities)`](#city_data) --> collects data for the various cities of interest via webscraping wikipedia.\n",
    "### [`population_update()`](#population_update) --> updates population data for the various cities of interest via webscraping wikipedia.\n",
    "### [`city_weather()`](#city_weather) --> collects weather data for the cities present in the local sql_Gans schema via the openweather API.\n",
    "### [`city_airports()`](#city_airports) --> collects data for the airports serving the various cities of interest.\n",
    "### [`city_flights()`](#city_flights) --> collects data for the flights arriving to the airports stored in the local sql_Gans schema via the aerodatabox API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4795c3-0f84-40d8-9207-93429cc3bec2",
   "metadata": {},
   "source": [
    "### The MySQL schema for sql_Gans.\n",
    "#### This code needs to be fed to the local MySQL instance.\n",
    "\n",
    "```sql\n",
    "-- Drop the database if it already exists\n",
    "DROP DATABASE IF EXISTS sql_Gans ;\n",
    "\n",
    "-- Create the database\n",
    "CREATE DATABASE sql_Gans;\n",
    "\n",
    "-- Use the database\n",
    "USE sql_Gans;\n",
    "\n",
    "-- Create the 'cities' table\n",
    "CREATE TABLE cities (\n",
    "    city_id INT AUTO_INCREMENT, -- Automatically generated ID for each city\n",
    "    city VARCHAR(255) NOT NULL, -- Name of the city\n",
    "    country VARCHAR(255) NOT NULL, -- Name of the city\n",
    "    PRIMARY KEY (city_id) -- Primary key to uniquely identify each city\n",
    ");\n",
    "\n",
    "-- creates \"coordinates\" table\n",
    "\n",
    "CREATE TABLE coordinates (\n",
    "    city_id INT AUTO_INCREMENT, -- ID of the city\n",
    "    latitude VARCHAR(255) NOT NULL, -- latitude of the city\n",
    "    longitude VARCHAR(255) NOT NULL, -- longitude of the city\n",
    "    FOREIGN KEY (city_id) REFERENCES cities(city_id) -- Foreign key to connect each population value to the respective city\n",
    ");\n",
    "\n",
    "-- create \"population\" table\n",
    "\n",
    "CREATE TABLE population (\n",
    "    id INT AUTO_INCREMENT, -- ID for each row\n",
    "    city_id INT, -- ID of the city\n",
    "    population INT UNSIGNED NOT NULL, -- Population of the city\n",
    "    retrieval_date DATE NOT NULL, -- date the population value was retrieved\n",
    "    PRIMARY KEY (id), -- Primary key to uniquely identify each row\n",
    "    FOREIGN KEY (city_id) REFERENCES cities(city_id) -- Foreign key to connect each population value to the respective city\n",
    ");\n",
    "\n",
    "-- creates \"weather_predictions\" table\n",
    "\n",
    "CREATE TABLE weather_predictions (\n",
    "    id INT AUTO_INCREMENT, -- ID for each row\n",
    "    city_id INT, -- ID of the city\n",
    "    forecast_time_UTC DATETIME NOT NULL, -- future timepoint the weather prediction pertains to\n",
    "    temperature FLOAT NOT NULL, -- temperature value\n",
    "    apparent_temperature FLOAT NOT NULL, -- approximate feeling of temperature\n",
    "    wind_speed FLOAT NOT NULL, -- wind speed\n",
    "    rain_last_3h_mm FLOAT NOT NULL, -- amount of rain that fell in the previous 3 hours\n",
    "    outlook VARCHAR(255) NOT NULL, -- how the weather conditions look like\n",
    "    retrieval_time_UTC DATETIME NOT NULL, -- timepoint of prediction data retrieval\n",
    "    PRIMARY KEY (id), -- Primary key to uniquely identify each row\n",
    "    FOREIGN KEY (city_id) REFERENCES cities(city_id) -- Foreign key to connect each population value to the respective city\n",
    ");\n",
    "\n",
    "-- creates \"airports\" table\n",
    "\n",
    "CREATE TABLE airports (\n",
    "    airport_icao VARCHAR(5) NOT NULL, -- ICAO code for the airport\n",
    "    airport_name VARCHAR(255) NOT NULL, -- name of the airport\n",
    "    PRIMARY KEY (airport_icao), -- Primary key to uniquely identify each row\n",
    "    UNIQUE (airport_icao) -- primary key columns can get only unique values\n",
    ");\n",
    "\n",
    "-- creates \"cities_airports\" table\n",
    "\n",
    "CREATE TABLE cities_airports (\n",
    "    id INT AUTO_INCREMENT, -- ID for each row\n",
    "    airport_icao VARCHAR(5) NOT NULL, -- ICAO code for the airport\n",
    "    city_id INT, -- ID of the city\n",
    "    PRIMARY KEY (id), -- Primary key to uniquely identify each row\n",
    "    FOREIGN KEY (city_id) REFERENCES cities(city_id), -- Foreign key to connect each airport to the respective city\n",
    "    FOREIGN KEY (airport_icao) REFERENCES airports(airport_icao) -- Foreign key to connect each airport to its full name\n",
    ");\n",
    "\n",
    "-- creates the \"flights\" table\n",
    "\n",
    "CREATE TABLE flights (\n",
    "    id INT AUTO_INCREMENT, -- Automatically generated ID for each flight\n",
    "    city_id INT, -- ID of the city\n",
    "    arrival_icao VARCHAR(5) NOT NULL, -- ICAO code for the airport near the city of interest\n",
    "    departure_icao VARCHAR(5), -- ICAO code for the airport where the flight is coming from\n",
    "    departure_name VARCHAR(255), -- name of the airport\n",
    "    flight_num VARCHAR(25) NOT NULL, -- identification number of the flight\n",
    "    arrival_time_UTC DATETIME NOT NULL, -- timepoint of landing    \n",
    "    retrieval_time_UTC DATETIME NOT NULL, -- timepoint of acquiring the data via the API\n",
    "    PRIMARY KEY (id), -- Primary key to uniquely identify each row\n",
    "    FOREIGN KEY (city_id) REFERENCES cities(city_id), -- Foreign key to connect each city-airport-flight combo to the respective city\n",
    "    FOREIGN KEY (arrival_icao) REFERENCES airports(airport_icao)\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15e819-341c-4eb9-ae0d-625078755d75",
   "metadata": {},
   "source": [
    "### Installing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5e7c6269-b273-449a-a20c-a2201cf3c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install if needed, depends on the local setup\n",
    "\n",
    "#!pip install sqlalchemy\n",
    "#!pip install pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a3188-c61e-40a4-99f3-f8f2dba30074",
   "metadata": {},
   "source": [
    "### Importing libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c16c6998-7385-427a-a7fc-fee0361cb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "from pytz import timezone\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5814a-058d-4062-9173-0d02e9a30cfc",
   "metadata": {},
   "source": [
    "### Credentials for connecting to the local MyQSL instance. <a id=\"local_mysql_Gans\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3691d9f5-4304-43f4-aaa7-3a8118147b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to connect to the sql_Gans schema (present in a local MySQL instance)\n",
    "\n",
    "def local_mysql_Gans():\n",
    "\n",
    "    schema = \"sql_Gans\"\n",
    "    host = \"127.0.0.1\"\n",
    "    user = \"root\"\n",
    "    password = \"PASSWORD_HERE\"\n",
    "    port = 3306\n",
    "    connection_string = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'\n",
    "    \n",
    "    return connection_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "089b5b39-6dfc-402c-969a-9d0ca7d9d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the credentials for connecting to the local sql_Gans schema in a variable that can be called in subsequent functions \n",
    "\n",
    "connection_string_Gans = local_mysql_Gans()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901725d6-9105-4f27-92e9-123df461cdd1",
   "metadata": {},
   "source": [
    "### The cities of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "502feef8-e467-49b4-9f41-93c241a6084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cities for which data will be scraped (a list is necessary to run the city_data() function)\n",
    "\n",
    "city_list = [\"Berlin\", \"Hamburg\", \"Munich\", \"Athens\", \"Manila\", \"London\", \"Paris\", \"Heidelberg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b161f-6003-473d-9289-21367a497731",
   "metadata": {},
   "source": [
    "### Function for webscraping data from wikipedia on the cities of interest. <a id=\"city_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ec3e9292-d6af-4b99-b19b-19e7d3798396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for webscraping data from wikipedia on various cities\n",
    "\n",
    "def city_data(cities):\n",
    "\n",
    "    # all data come from wikipedia\n",
    "    link_prefix = \"https://en.wikipedia.org/wiki/\"\n",
    "\n",
    "    # an empty dictionary for storing the data retrieved from wikipedia \n",
    "    cities_data = {}\n",
    "\n",
    "    # storing the date on which the data on the various cities were retrieved \n",
    "    retrieval_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    for city in cities:\n",
    "\n",
    "        # creation of the url to be scraped.\n",
    "        url = link_prefix + city\n",
    "    \n",
    "        # GET http request.\n",
    "        response = requests.get(url)\n",
    "    \n",
    "        # applying some BeautifulSoup magic.\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # finding all html elements that store population values in the upper right table of the respective Wikipedia page and fetching the first element that corresponds to the city value.\n",
    "        population = soup.find_all('td', class_ = 'infobox-data', string = re.compile(\"[0-9]{1,3},*[0-9]{1,3},[0-9]{1,3}\"))[0].get_text()\n",
    "    \n",
    "        # finding all html elements that have the text \"Country\" in the upper right table of the respective Wikipedia page (there is only a single element) and fetching the next element that corresponds to the country. \n",
    "        country = soup.find_all('th', class_ = 'infobox-label', string = \"Country\")[0].find_next(\"td\").get_text()\n",
    "    \n",
    "        # making webscraping function more robust by removing non-breaking spaces present in some cases e.g. Philippines\n",
    "        country = country.replace(u\"\\xa0\", u\"\")\n",
    "        \n",
    "        # finding the html element that stores the latitude of the city. \n",
    "        latitude = soup.find_all('span', class_ = 'latitude')[0].get_text()\n",
    "    \n",
    "        # finding the html element that stores the longitude of the city.\n",
    "        longitude = soup.find_all('span', class_ = 'longitude')[0].get_text()\n",
    "    \n",
    "        # populates the cities_data dictionary with the requested details for each city\n",
    "        cities_data[city] = [population, country, latitude, longitude, retrieval_date]\n",
    "\n",
    "        # control point\n",
    "        #print(f\"{city} has {population} inhabitants and is located in {country} at {latitude} {longitude}.\")\n",
    "\n",
    "    # converts the cities_data dictionary to a data frame\n",
    "    cities_df = (pd.DataFrame.from_dict(cities_data, orient = \"index\", columns=[\"Population\", \"Country\", \"Latitude\", \"Longitude\", \"retrieval_date\"])\n",
    "                   .reset_index()\n",
    "                   .rename(columns = {\"index\" : \"City\"})\n",
    "                )\n",
    "    \n",
    "    # removes commas from the population values\n",
    "    cities_df[\"Population\"] = pd.to_numeric(cities_df[\"Population\"].str.replace(\",\", \"\"))\n",
    "    \n",
    "    # changes \"retrieval_date\" column to datetime\n",
    "    cities_df[\"retrieval_date\"] = pd.to_datetime(cities_df[\"retrieval_date\"])\n",
    "\n",
    "    # sending info to the \"cities\" table of the local MySQL sql_Gans schema\n",
    "    # \"cities\" is updated only via this function and \"city_id\" column in the \"cities\" table is auto generated\n",
    "    pd.DataFrame(cities_df[[\"City\", \"Country\"]]).to_sql('cities',\n",
    "                                                        if_exists='append',\n",
    "                                                        con=connection_string_Gans,\n",
    "                                                        index=False\n",
    "                                                       )\n",
    "    \n",
    "    # sending info to the \"coordinates\" table of the local MySQL sql_Gans schema\n",
    "    # \"coordinates\" is updated only via this function and \"city_id\" column in the \"coordinates\" is auto generated\n",
    "    pd.DataFrame(cities_df[[\"Latitude\", \"Longitude\"]]).to_sql('coordinates',\n",
    "                                                              if_exists='append',\n",
    "                                                              con=connection_string_Gans,\n",
    "                                                              index=False\n",
    "                                                             )\n",
    "\n",
    "    # reading the \"city_id\" values present in the sql_Gans schema in order to pass them on to the \"population\" table\n",
    "    # this must happen in a loop for the cities added in each function run, otherwise the \"population\" table will always get \"city_id\" values starting from zero\n",
    "    cities_sql_df = pd.read_sql('''\n",
    "                                select * from cities\n",
    "                                ''',\n",
    "                                con=connection_string_Gans\n",
    "                               )\n",
    "\n",
    "    # creation of an empty data frame that will store the \"city_id values\" for the cities added in each function run    \n",
    "    cities_id_df = pd.DataFrame()\n",
    "    \n",
    "    # accessing the \"city_id\" value for each city present in the sql_Gans schema (for the cities added in each function run)\n",
    "    for city in cities:\n",
    "        city_id_df = pd.DataFrame(cities_sql_df.loc[cities_sql_df[\"city\"] == city, \"city_id\"])    \n",
    "        cities_id_df = pd.concat([cities_id_df, city_id_df], ignore_index = True)\n",
    "    \n",
    "    # inserting the \"city_id\" values from the \"cities\" table of the sql_Gans schema in the \"cities_df\" dataframe\n",
    "    cities_df.insert(0, \"city_id\", cities_id_df[\"city_id\"])\n",
    "        \n",
    "    # sending info to the \"population\" table of the local MySQL sql_Gans schema\n",
    "    # \"population\" will be also updated via code placed outside of the present function (see population_update() function)\n",
    "    pd.DataFrame(cities_df[[\"city_id\", \"Population\", \"retrieval_date\"]]).to_sql('population',\n",
    "                                                                                if_exists='append',\n",
    "                                                                                con=connection_string_Gans,\n",
    "                                                                                index=False\n",
    "                                                                               )\n",
    "    \n",
    "    return \"Tables \\\"cities\\\", \\\"coordinates\\\" and \\\"population\\\" have been updated.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfbbbc4-4ddf-4b7c-89c8-cde24aae0a1a",
   "metadata": {},
   "source": [
    "### First run of the city_data() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c0608807-d86f-4ff7-8603-97b94ece11fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tables \"cities\", \"coordinates\" and \"population\" have been updated.'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_data(city_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e78f9-657b-43a4-bedc-695c5b52a903",
   "metadata": {},
   "source": [
    "### More cities of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "160be92b-a295-4e49-9c40-2896ca9e0bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more cities for which data will be scraped (a list is necessary to run the city_data() function)\n",
    "\n",
    "city_list_2 = [\"Frankfurt\", \"Madrid\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221a805-28eb-4e16-ba61-a04da8a76404",
   "metadata": {},
   "source": [
    "### Second run of the city_data() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "61bcd4bb-b4df-4480-8fc3-eafaaf34b2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tables \"cities\", \"coordinates\" and \"population\" have been updated.'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_data(city_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c415885-d369-41f8-87fc-653ea4b598d3",
   "metadata": {},
   "source": [
    "### Function for updating the population of the cities of interest. <a id=\"population_update\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "72b3f38d-9470-4e36-b80a-8758a0a83d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for updating the population values for cities already present in the local sql_Gans schema via webscraping from wikipedia\n",
    "\n",
    "def population_update():\n",
    "\n",
    "    # reading the cities present in the sql_Gans schema\n",
    "    cities_from_sql_df = pd.read_sql('''\n",
    "                                     select * from cities\n",
    "                                     ''',\n",
    "                                     con=connection_string_Gans\n",
    "                                     )\n",
    "\n",
    "    # storing the cities retrieved from sql_Gans into a list\n",
    "    cities_list = cities_from_sql_df[\"city\"].to_list()\n",
    "\n",
    "    # storing the city IDs retrieved from sql_Gans into a list\n",
    "    city_id_list = cities_from_sql_df[\"city_id\"].to_list()\n",
    "\n",
    "    # all data come from wikipedia\n",
    "    link_prefix = \"https://en.wikipedia.org/wiki/\"\n",
    "\n",
    "    # an empty dictionary for storing the data retrieved from wikipedia \n",
    "    cities_data = {}\n",
    "\n",
    "    # storing the date on which the data on the various cities were retreived \n",
    "    retrieval_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # zip() pulls together each pair of city IDs and city names in a tuple \n",
    "    for city in [x for x in zip(city_id_list, cities_list)]:\n",
    "\n",
    "        # creation of the url to be scraped.\n",
    "        url = link_prefix + city[1]\n",
    "    \n",
    "        # GET http request.\n",
    "        response = requests.get(url)\n",
    "    \n",
    "        # applying some BeautifulSoup magic.\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "        # finding all html elements that store population values in the upper right table of the respective Wikipedia page and fetching the first element that corresponds to the city value.\n",
    "        population = soup.find_all('td', class_ = 'infobox-data', string = re.compile(\"[0-9]{1,3},*[0-9]{1,3},[0-9]{1,3}\"))[0].get_text()\n",
    "    \n",
    "        # populates the cities_data dictionary with the requested details for each city\n",
    "        cities_data[city[1]] = [city[0], population, retrieval_date]\n",
    "\n",
    "        # control point\n",
    "        #print(f\"{city[1]} has {population} inhabitants.\")\n",
    "        \n",
    "    # converts the cities_data dictionary to a data frame\n",
    "    cities_df = (pd.DataFrame.from_dict(cities_data, orient = \"index\", columns=[\"city_id\", \"Population\", \"retrieval_date\"])\n",
    "                   .reset_index()\n",
    "                   .rename(columns = {\"index\" : \"City\"})\n",
    "                )\n",
    "    \n",
    "    # removes commas from the population values\n",
    "    cities_df[\"Population\"] = pd.to_numeric(cities_df[\"Population\"].str.replace(\",\", \"\"))\n",
    "    \n",
    "    # changes \"retrieval_date\" column to datetime\n",
    "    cities_df[\"retrieval_date\"] = pd.to_datetime(cities_df[\"retrieval_date\"])\n",
    "\n",
    "    # sending info to the \"population\" table of the local MySQL sql_Gans schema\n",
    "    pd.DataFrame(cities_df[[\"city_id\", \"Population\", \"retrieval_date\"]]).to_sql('population',\n",
    "                                                                                if_exists='append',\n",
    "                                                                                con=connection_string_Gans,\n",
    "                                                                                index=False\n",
    "                                                                               )\n",
    "    \n",
    "    return \"Table \\\"population\\\" has been updated.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f1f06a-20ba-4cf1-a20c-24608d7dc30f",
   "metadata": {},
   "source": [
    "### Updating the population values in the local MySQL schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "330db7e8-3c63-4e7b-be4f-391cb48e1e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table \"population\" has been updated.'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e72ba-654b-4c00-a0d9-c2adc69d9861",
   "metadata": {},
   "source": [
    "### Function for collecting weather data for the cities of interest. <a id=\"city_weather\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6c05f005-c596-4e18-8680-3c6e4a33f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect weather data from the openweather API for the cities ALREADY present in the local sql_Gans schema and insert them back into the local sql_Gans schema\n",
    "\n",
    "def city_weather():\n",
    "\n",
    "    # reading the cities present in the sql_Gans schema\n",
    "    cities_from_sql_df = pd.read_sql('''\n",
    "                                     select * from cities\n",
    "                                     ''',\n",
    "                                     con=connection_string_Gans\n",
    "                                     )\n",
    "\n",
    "    # storing the cities retrieved from sql_Gans into a list\n",
    "    cities_list = cities_from_sql_df[\"city\"].to_list()\n",
    "\n",
    "    # storing the city IDs retrieved from sql_Gans into a list\n",
    "    city_id_list = cities_from_sql_df[\"city_id\"].to_list()\n",
    "\n",
    "    # creation of an empty data frame that will store the final result\n",
    "    cities_weather_df = pd.DataFrame()\n",
    "\n",
    "    # storing the time point (UTC) on which the data on the various cities were retrieved\n",
    "    # openweather api reports weather forecast values for time point in UTC only\n",
    "    utc_timezone = timezone('UTC')\n",
    "    retrieval_time = datetime.now(utc_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # key for accessing the openweather API\n",
    "    API_key = \"API_KEY_HERE\"\n",
    "\n",
    "    # zip() pulls together each pair of city IDs and city names in a tuple \n",
    "    for city in [x for x in zip(city_id_list, cities_list)]:\n",
    "        \n",
    "        # fetching weather data for a city from openweather\n",
    "        forecast = requests.get(f\"http://api.openweathermap.org/data/2.5/forecast?q={city[1]}&appid={API_key}&units=metric\")\n",
    "\n",
    "        # extracting info from the openweather response as a json\n",
    "        forecast_json = forecast.json()\n",
    "\n",
    "        # normalizing the \"list\" key of the forecast_json (contains the weather data) and creating a data frame that contains the weather data\n",
    "        forecast_df = pd.json_normalize(forecast_json[\"list\"])\n",
    "        \n",
    "        # extracting the weather evaluation from the \"weather\" column in order to create the \"outlook\" column\n",
    "        forecast_df[\"outlook\"] = [forecast_df[\"weather\"][x][0][\"main\"] for x in range(len(forecast_df[\"weather\"]))]\n",
    "\n",
    "        # replacing NaN values in the rain propability column with 0 BUT only if the \"rain.3h\" column is already present\n",
    "        # sometimes \"rain.3h\" values are not reported in the query, THEN the \"rain.3h\" column is added and populated with zeros\n",
    "        if \"rain.3h\" in forecast_df.columns:\n",
    "            forecast_df[\"rain.3h\"] = forecast_df[\"rain.3h\"].fillna(0)\n",
    "        else:\n",
    "            forecast_df[\"rain.3h\"] = [0 for _ in range(len(forecast_df.index))]     \n",
    "        \n",
    "        # inserting the time point on which each weather prediction was retrieved\n",
    "        forecast_df[\"retrieval_time_UTC\"] = [retrieval_time for _ in range(len(forecast_df.index))]\n",
    "        \n",
    "        # cleaning up the forecast_df data frame by dropping columns with unnecessary data\n",
    "        forecast_df.drop([name for name in forecast_df.columns if name not in [\"dt_txt\", \"main.temp\", \"main.feels_like\", \"wind.speed\", \"rain.3h\", \"outlook\", \"retrieval_time_UTC\"]], axis=1, inplace=True)\n",
    "\n",
    "        # renaming the columns of the data frame\n",
    "        # openweather api reports weather forecast values for timpe point in UTC only\n",
    "        forecast_df.rename(columns = {\"dt_txt\" : \"forecast_time_UTC\",\n",
    "                                      \"main.temp\" : \"temperature\",\n",
    "                                      \"main.feels_like\" : \"apparent_temperature\",\n",
    "                                      \"wind.speed\" : \"wind_speed\",\n",
    "                                      \"rain.3h\" : \"rain_last_3h_mm\",\n",
    "                                      \"outlook\" : \"outlook\",\n",
    "                                      \"retrieval_time_UTC\" : \"retrieval_time_UTC\"\n",
    "                                     }, inplace = True\n",
    "                          )\n",
    "        \n",
    "        # inserting the \"city_id\" value from MySQL into the \"forecast\" data frame in a new column also named \"city_id\"\n",
    "        forecast_df.insert(0, \"city_id\", [city[0] for _ in range(len(forecast_df.index))])\n",
    "        \n",
    "        # changing \"forecast_time_UTC\" column to datetime\n",
    "        forecast_df[\"forecast_time_UTC\"] = pd.to_datetime(forecast_df[\"forecast_time_UTC\"])\n",
    "\n",
    "        # changing \"retrieval_time_UTC\" column to datetime\n",
    "        forecast_df[\"retrieval_time_UTC\"] = pd.to_datetime(forecast_df[\"retrieval_time_UTC\"])\n",
    "        \n",
    "        # inserting the weather values from forecast_df into the collective cities_weather_df data frame\n",
    "        cities_weather_df = pd.concat([cities_weather_df, forecast_df], ignore_index = True)\n",
    "        \n",
    "    # inserting the collected weather values for all cities into the Gans MySQL schema\n",
    "    cities_weather_df.to_sql('weather_predictions',\n",
    "                             if_exists='append',\n",
    "                             con=connection_string_Gans,\n",
    "                             index=False\n",
    "                            )\n",
    "    \n",
    "    return \"Table \\\"weather_predictions\\\" has been updated.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569126d4-b3e4-44b4-bec7-a63491841146",
   "metadata": {},
   "source": [
    "### Collecting the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "5912ba94-e673-4bb3-b180-d903ac40341a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table \"weather_predictions\" has been updated.'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_weather()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14afa1ba-8482-4c89-b9de-366a01f665e3",
   "metadata": {},
   "source": [
    "### Function for collecting airport data for the cities of interest. <a id=\"city_airports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "772b913f-b1a4-468e-9cd2-98c5a86085e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to collect data for the airports serving the cities ALREADY present in the local sql_Gans schema and insert them back into the local sql_Gans schema\n",
    "# CAUTION: the tables \"airports\" and \"cities_airports\" of the sql_Gans schema must be dropped and recreated before each run of this function!!!!!!!!!!!!!\n",
    "\n",
    "def city_airports():\n",
    " \n",
    "    # reading the cities present in the sql_Gans schema\n",
    "    cities_from_sql_df = (pd.read_sql('''\n",
    "                                      select * from cities\n",
    "                                      ''',\n",
    "                                      con=connection_string_Gans\n",
    "                                     )\n",
    "                         )\n",
    "\n",
    "    # storing the cities retrieved from MySQL into a list (order retained)\n",
    "    cities_list = cities_from_sql_df[\"city\"].to_list()\n",
    "\n",
    "    # retrieving the coordinates for the cities in the Gans database\n",
    "    city_coordinates_from_sql_df = (pd.read_sql('''\n",
    "                                                select * from coordinates\n",
    "                                                ''',\n",
    "                                                con=connection_string_Gans\n",
    "                                               )\n",
    "                                   )\n",
    "    # adding a column that will help allocate the correct negative or positive value to the latitude of each city\n",
    "    city_coordinates_from_sql_df[\"latitude_description\"] = city_coordinates_from_sql_df.apply(lambda row: \"north\" if \"N\" in row[\"latitude\"] else \"south\",  axis = 1)\n",
    "\n",
    "    # adding a column that will help allocate the correct negative or positive value to the longitude of each city\n",
    "    city_coordinates_from_sql_df[\"longitude_description\"] = city_coordinates_from_sql_df.apply(lambda row: \"east\" if \"E\" in row[\"longitude\"] else \"west\",  axis = 1)\n",
    "\n",
    "    # transforming the \"latitude\" column from string to numeric\n",
    "    # CAUTION: the .str.split() below creates lists for each element of [\"latitude\"] that have empty elements in them\n",
    "    # HOWEVER the transformation from string to numeric is successful and the required info is preserved\n",
    "    city_coordinates_from_sql_df[\"latitude\"] = pd.to_numeric(city_coordinates_from_sql_df.loc[:, \"latitude\"]\n",
    "                                                                                         .str.split(r\"\\D\", regex = True)\n",
    "                                                                                         .apply(lambda x: x[0] + \".\" + x[1] + x[2])\n",
    "                                                            )\n",
    "\n",
    "    # transforming the \"latitude\" into a negative number if the respective city is located south of the equator\n",
    "    city_coordinates_from_sql_df[\"latitude\"] = city_coordinates_from_sql_df.apply(lambda row: -abs(row[\"latitude\"]) if row[\"latitude_description\"] == \"south\" else row[\"latitude\"], axis = 1)\n",
    "\n",
    "    # transforming the \"longitude\" column from string to numeric\n",
    "    # CAUTION: the .str.split() below creates lists for each element of [\"longitude\"] that have empty elements in them\n",
    "    # HOWEVER the transformation from string to numeric is successful and the required info is preserved\n",
    "    city_coordinates_from_sql_df[\"longitude\"] = pd.to_numeric(city_coordinates_from_sql_df.loc[:, \"longitude\"]\n",
    "                                                                                          .str.split(r\"\\D\", regex = True)\n",
    "                                                                                          .apply(lambda x: x[0] + \".\" + x[1] + x[2])\n",
    "                                                             )\n",
    "    \n",
    "    # transforming the \"longitude\" into a negative number if the respective city is located west of the Greenwich meridian\n",
    "    city_coordinates_from_sql_df[\"longitude\"] = city_coordinates_from_sql_df.apply(lambda row: -abs(row[\"longitude\"]) if row[\"longitude_description\"] == \"west\" else row[\"longitude\"], axis = 1)\n",
    "\n",
    "    # merging cities_from_sql_df and city_coordinates_from_sql_df\n",
    "    cities_full_df = cities_from_sql_df.merge(city_coordinates_from_sql_df, how = \"left\", on = \"city_id\")\n",
    "\n",
    "    # removing unnecessary columns from the \"cities_full_df\" data frame\n",
    "    cities_full_df.drop([\"latitude_description\", \"longitude_description\"], axis=1, inplace=True)\n",
    "\n",
    "    # assembling a list with city coordinates (latitude, longitude)\n",
    "    cities_coordinates_list = [x for x in zip(cities_full_df[\"city_id\"], cities_full_df[\"latitude\"], cities_full_df[\"longitude\"])]\n",
    "    \n",
    "    # creation of an empty data frame that will store the airports, which serve each city\n",
    "    all_airports_df = pd.DataFrame()\n",
    "    \n",
    "    # iterating through each city's coordinates and collecting airport data for each one\n",
    "    for element in cities_coordinates_list:\n",
    "        \n",
    "        # finding airports near the cities present in the Gans database through the aerodatabox API\n",
    "        airport_url = \"https://aerodatabox.p.rapidapi.com/airports/search/location\"\n",
    "        \n",
    "        airport_querystring = {\"lat\" : element[1], \"lon\": element[2], \"radiusKm\" : \"90\", \"limit\" : \"10\", \"withFlightInfoOnly\" : \"true\"}\n",
    "\n",
    "        airport_headers = {\"X-RapidAPI-Key\": \"API_KEY_HERE\",\n",
    "\t                       \"X-RapidAPI-Host\": \"aerodatabox.p.rapidapi.com\"\n",
    "                          }\n",
    "\n",
    "        airports = requests.get(airport_url, headers=airport_headers, params=airport_querystring)\n",
    "\n",
    "        airports_json = airports.json()\n",
    "\n",
    "        airports_df = pd.json_normalize(airports_json[\"items\"])\n",
    "        \n",
    "        # renaming some columns of the \"airports_df\" data frame\n",
    "        airports_df.rename(columns = {\"icao\" : \"airport_icao\",\n",
    "                                      \"name\" : \"airport_name\",\n",
    "                                     }, inplace = True\n",
    "                          )\n",
    "        \n",
    "        # inserting the \"city_id\" value from MySQL into the \"airports\" data frame in a new column also named \"city_id\"\n",
    "        airports_df.insert(0, \"city_id\", [element[0] for _ in range(len(airports_df.index))])\n",
    "        \n",
    "        # inserting the airport data from airports_df into the collective all_airports_df data frame\n",
    "        all_airports_df = pd.concat([all_airports_df, airports_df], ignore_index = True)\n",
    "\n",
    "    # filtering out duplicate entries since some airports might serve more than one city\n",
    "    unique_airports_df = all_airports_df.loc[:, [\"airport_icao\", \"airport_name\"]].drop_duplicates(subset = [\"airport_icao\"], ignore_index=True)\n",
    "    \n",
    "\n",
    "    # inserting the unique airports into the local MySQL sql_Gans schema (\"airports\" table)\n",
    "    pd.DataFrame(unique_airports_df[[\"airport_icao\", \"airport_name\"]]).to_sql('airports',\n",
    "                                                                              if_exists='append',\n",
    "                                                                              con=connection_string_Gans,\n",
    "                                                                              index=False\n",
    "                                                                             )\n",
    "    \n",
    "    # inserting the collected airport data into the local MySQL sql_Gans schema (\"cities airports\" table)\n",
    "    pd.DataFrame(all_airports_df[[\"city_id\", \"airport_icao\"]]).to_sql('cities_airports',\n",
    "                                                                      if_exists='append',\n",
    "                                                                      con=connection_string_Gans,\n",
    "                                                                      index=False\n",
    "                                                                     )\n",
    "\n",
    "    return \"Tables \\\"airports\\\" and \\\"cities_airports\\\" have been updated.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5804576e-656a-43af-8290-3cea41fa8c2f",
   "metadata": {},
   "source": [
    "### Collecting airport names and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "ec16dfd5-67de-4c75-9075-ad2082b2d91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tables \"airports\" and \"cities_airports\" have been updated.'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_airports()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d72261-5be7-4bec-a186-598d116a63f9",
   "metadata": {},
   "source": [
    "### Function for collecting flight data from the airports near the cities of interest. <a id=\"city_flights\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "82c5ebad-0ccb-48b9-95fc-404e70d6686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function to collect data for tomorrow's incoming flights to the airports serving the cities ALREADY present in the local sql_Gans schema and insert them back into the local sql_Gans schema\n",
    "# the aerodatabox API returns time in UTC format or in the local timezone of the computer making the request\n",
    "\n",
    "def city_flights():\n",
    "\n",
    "    # reading the cities and associated airports present in the sql_Gans schema\n",
    "    # note that some airports might serve more than one city\n",
    "    cities_and_airports_from_sql_df = (pd.read_sql('''\n",
    "                                                   select city_id, airport_icao from cities_airports\n",
    "                                                   ''',\n",
    "                                                   con=connection_string_Gans\n",
    "                                                  )\n",
    "                                      )\n",
    "\n",
    "    # storing the cities retrieved from MySQL into a list\n",
    "    cities_and_airports_list = list(zip(cities_and_airports_from_sql_df[\"city_id\"], cities_and_airports_from_sql_df[\"airport_icao\"]))\n",
    "\n",
    "    # creation of an empty data frame that will store the final result\n",
    "    all_arrivals_df = pd.DataFrame()\n",
    "\n",
    "    # storing tomorrow's date in a string\n",
    "    tomorrow = (date.today() + timedelta(1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # storing the time point (UTC) on which the data on the various flights were retrieved\n",
    "    # the UTC format of the aerodatabox API response is used in this function\n",
    "    utc_timezone = timezone('UTC')\n",
    "    retrieval_time = datetime.now(utc_timezone).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # gathering data on the arriving flights to the airports near the cities of interest\n",
    "    for element in cities_and_airports_list:\n",
    "\n",
    "        # finding arriving flights near the cities present in the sql_Gans schema through the aerodatabox API\n",
    "        # api url for the first half of the day (api query time ranges are limited to 12 hours)\n",
    "        url1 = f\"https://aerodatabox.p.rapidapi.com/flights/airports/icao/{element[1]}/{tomorrow}T00:00/{tomorrow}T11:59\"\n",
    "\n",
    "        # api url for the second half of the day (api query time ranges are limited to 12 hours)\n",
    "        url2 = f\"https://aerodatabox.p.rapidapi.com/flights/airports/icao/{element[1]}/{tomorrow}T12:00/{tomorrow}T23:59\"\n",
    "\n",
    "        # search is done only for arriving flights\n",
    "        querystring = {\"withLeg\":\"true\",\"direction\":\"Arrival\",\"withCancelled\":\"false\",\"withCodeshared\":\"true\",\"withCargo\":\"false\",\"withPrivate\":\"true\",\"withLocation\":\"false\"}\n",
    "\n",
    "        headers = {\"X-RapidAPI-Key\": \"API_KEY_HERE\",\n",
    "    \t           \"X-RapidAPI-Host\": \"aerodatabox.p.rapidapi.com\"\n",
    "                  }\n",
    "\n",
    "        # the html response for the first half of the day\n",
    "        response1 = requests.get(url1, headers=headers, params=querystring)\n",
    "\n",
    "        # the html response for the second half of the day\n",
    "        response2 = requests.get(url2, headers=headers, params=querystring)\n",
    "\n",
    "        # combining html responses for tomorrow in a list\n",
    "        response_list = [response1, response2]\n",
    "\n",
    "        # going through the full html response in search of flight data\n",
    "        for response in response_list:\n",
    "\n",
    "            # only successful get requests are considered (airports will report 204 if no flights are retrieved for the search criteria above)\n",
    "            if response.status_code == 200:\n",
    "        \n",
    "                arrivals_df = pd.json_normalize(response.json()[\"arrivals\"])\n",
    "        \n",
    "                # cleaning up the arrivals_df data frame by dropping columns with unnecessary data\n",
    "                arrivals_df.drop([name for name in arrivals_df.columns if name not in [\"number\", \"departure.airport.icao\", \"departure.airport.name\", \"arrival.scheduledTime.utc\"]], axis=1, inplace=True)\n",
    "\n",
    "                # renaming the columns of the data frame\n",
    "                arrivals_df.rename(columns = {\"number\" : \"flight_num\",\n",
    "                                              \"departure.airport.icao\" : \"departure_icao\",\n",
    "                                              \"departure.airport.name\" : \"departure_name\",\n",
    "                                              \"arrival.scheduledTime.utc\" : \"arrival_time_UTC\"\n",
    "                                             }, inplace = True\n",
    "                                  )\n",
    "\n",
    "                # ordering the columns in data frame (useful for depicting the \"arrivals_df\" data frame in a jupyter notebook)\n",
    "                arrivals_df = arrivals_df.reindex(columns=[\"departure_icao\", \"departure_name\", \"flight_num\", \"arrival_time_UTC\"])\n",
    "\n",
    "                # inserting the \"airport_icao\" value from MySQL into the \"arrivals\" data frame in a new column also named \"arrival_icao\"\n",
    "                arrivals_df.insert(0, \"arrival_icao\", [element[1] for _ in range(len(arrivals_df.index))])\n",
    "\n",
    "                # inserting the \"city_id\" value from MySQL into the \"arrivals\" data frame in a new column also named \"city_id\"\n",
    "                arrivals_df.insert(0, \"city_id\", [element[0] for _ in range(len(arrivals_df.index))])\n",
    "\n",
    "                # inserting the time point on which each arrival time was retrieved\n",
    "                arrivals_df[\"retrieval_time_UTC\"] = [retrieval_time for _ in range(len(arrivals_df.index))]\n",
    "                        \n",
    "                # changing \"arrival_time_UTC\" column to datetime\n",
    "                arrivals_df[\"arrival_time_UTC\"] = pd.to_datetime(arrivals_df[\"arrival_time_UTC\"])\n",
    "\n",
    "                # changing \"retrieval_time_UTC\" column to datetime\n",
    "                arrivals_df[\"retrieval_time_UTC\"] = pd.to_datetime(arrivals_df[\"retrieval_time_UTC\"])\n",
    "        \n",
    "                # inserting the arrivals data from arrivals_df into the collective \"all_arrivals_df\" data frame\n",
    "                all_arrivals_df = pd.concat([all_arrivals_df, arrivals_df], ignore_index = True)\n",
    "\n",
    "            # if the get request for flight data failed, the next airport in the list is probed\n",
    "            else:\n",
    "\n",
    "                continue\n",
    "                \n",
    "    # inserting the collected airport data into the local sql_Gans schema (\"cities airports\" table)\n",
    "    all_arrivals_df.to_sql('flights',\n",
    "                           if_exists='append', \n",
    "                           con=connection_string_Gans, \n",
    "                           index=False)\n",
    "    \n",
    "    return \"Table \\\"flights\\\" has been updated.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc13ac-f2ee-4a26-be32-ad7548424ce2",
   "metadata": {},
   "source": [
    "### Collecting data on inbound flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "03719b5b-9cb9-4f10-a20f-966bb1852f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table \"flights\" has been updated.'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_flights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
